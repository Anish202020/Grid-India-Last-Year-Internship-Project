{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61f21f2-f671-40f5-89ca-e13f5e3cd5cb",
   "metadata": {},
   "source": [
    "# LSTM MODEL CREATION FOR FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221454f1-919a-4290-9e9f-897099bce659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\College\\7th Semester\\Internship\\Application\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 25ms/step - loss: 0.0263\n",
      "Epoch 2/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 24ms/step - loss: 0.0034\n",
      "Epoch 3/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 22ms/step - loss: 0.0032\n",
      "Epoch 4/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1483s\u001b[0m 126ms/step - loss: 0.0030\n",
      "Epoch 5/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 20ms/step - loss: 0.0030\n",
      "Epoch 6/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 19ms/step - loss: 0.0029\n",
      "Epoch 7/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 22ms/step - loss: 0.0028\n",
      "Epoch 8/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 23ms/step - loss: 0.0028\n",
      "Epoch 9/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 23ms/step - loss: 0.0028\n",
      "Epoch 10/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 33ms/step - loss: 0.0028\n",
      "Epoch 11/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 18ms/step - loss: 0.0028\n",
      "Epoch 12/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 20ms/step - loss: 0.0028\n",
      "Epoch 13/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 20ms/step - loss: 0.0028\n",
      "Epoch 14/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 23ms/step - loss: 0.0028\n",
      "Epoch 15/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 23ms/step - loss: 0.0028\n",
      "Epoch 16/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 21ms/step - loss: 0.0028\n",
      "Epoch 17/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 19ms/step - loss: 0.0028\n",
      "Epoch 18/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 19ms/step - loss: 0.0028\n",
      "Epoch 19/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 19ms/step - loss: 0.0028\n",
      "Epoch 20/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 19ms/step - loss: 0.0027\n",
      "Epoch 21/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 19ms/step - loss: 0.0027\n",
      "Epoch 22/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 19ms/step - loss: 0.0027\n",
      "Epoch 23/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 19ms/step - loss: 0.0028\n",
      "Epoch 24/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 19ms/step - loss: 0.0027\n",
      "Epoch 25/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 19ms/step - loss: 0.0027\n",
      "Epoch 26/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 19ms/step - loss: 0.0028\n",
      "Epoch 27/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 19ms/step - loss: 0.0028\n",
      "Epoch 28/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 20ms/step - loss: 0.0028\n",
      "Epoch 29/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 20ms/step - loss: 0.0027\n",
      "Epoch 30/30\n",
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 20ms/step - loss: 0.0027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c298890d10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras._tf_keras.keras.models import Sequential \n",
    "from keras._tf_keras.keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "def preprocess_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        vrm_data = df['VRM'].values.reshape(-1, 1)\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if np.any(np.isnan(vrm_data)):\n",
    "            print(f\"NaN values found in {file_path}.\")\n",
    "            # Check if there are any valid values to compute the mean\n",
    "            if np.count_nonzero(~np.isnan(vrm_data)) > 0:\n",
    "                mean_value = np.nanmean(vrm_data)\n",
    "                print(f\"Filling NaNs with the mean: {mean_value}\")\n",
    "                vrm_data = np.nan_to_num(vrm_data, nan=mean_value)\n",
    "            else:\n",
    "                print(f\"All values are NaN in {file_path}. Skipping this file.\")\n",
    "                return None  # Skip this file if all values are NaN\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        vrm_data_scaled = scaler.fit_transform(vrm_data)\n",
    "        return vrm_data_scaled\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fourier_transform(data):\n",
    "    fft_data = np.fft.fft(data)\n",
    "    fft_data = np.abs(fft_data)\n",
    "    return fft_data\n",
    "\n",
    "folder_path = 'VRM_data'\n",
    "\n",
    "processed_data = []\n",
    "file_names = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        data = preprocess_csv(file_path)\n",
    "        if data is not None:\n",
    "            processed_data.append(data)\n",
    "            file_names.append(filename)\n",
    "\n",
    "\n",
    "\n",
    "sequence_length = 30\n",
    "# Preparing Data for LSTM\n",
    "sequence_length = 30\n",
    "X = []\n",
    "for data in processed_data:\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "\n",
    "# Convert to NumPy array and reshape\n",
    "X = np.array(X)\n",
    "X = X.reshape(X.shape[0], sequence_length, 1)  # Reshape to (number_of_samples, sequence_length, num_features)\n",
    "\n",
    "# Check for NaN values in X\n",
    "if np.any(np.isnan(X)):\n",
    "    print(\"NaN values found in X. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Create a more complex LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(sequence_length, 1)))  # First LSTM layer\n",
    "model.add(Dropout(0.2))  # Dropout layer to prevent overfitting\n",
    "model.add(LSTM(50, return_sequences=True))  # Second LSTM layer\n",
    "model.add(Dropout(0.2))  # Another Dropout layer\n",
    "model.add(LSTM(25))  # Third LSTM layer\n",
    "model.add(Dropout(0.2))  # Dropout layer\n",
    "model.add(Dense(30))  # Output layer\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, X, epochs=30, batch_size=32)  # Increased epochs for better training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a853d6-a8ca-487d-99da-76b1f50b8767",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION\n",
    "## Two Features\n",
    "### LSTM & Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632cefe5-d883-4413-8fe3-73ac3c32b764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11789/11789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict LSTM features\n",
    "\n",
    "lstm_features = model.predict(X)\n",
    "lstm_features = lstm_features.reshape(lstm_features.shape[0], -1)\n",
    "fourier_features = [fourier_transform(data.flatten()) for data in processed_data]\n",
    "\n",
    "# Combine features for clustering\n",
    "combined_features = []\n",
    "for lstm_f, fourier_f in zip(lstm_features, fourier_features):\n",
    "    combined_features.append(np.concatenate((lstm_f, fourier_f[:10])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e96140-7b33-4fc2-975b-a143acc42e6c",
   "metadata": {},
   "source": [
    "# K Means Clustering\n",
    "### 10 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b790524-6056-48d5-91d0-d102b250829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files successfully clustered.\n"
     ]
    }
   ],
   "source": [
    "# KMeans clustering\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "kmeans.fit(combined_features)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Create directories for clusters\n",
    "os.makedirs('Signature Fault Clusters Version 16 Final/VRM', exist_ok=True)\n",
    "for i in range(10):\n",
    "    os.makedirs(os.path.join('Signature Fault Clusters Version 16 Final/VRM', f'VRM Cluster {i}'), exist_ok=True)\n",
    "\n",
    "# Move files to their respective clusters\n",
    "for i, filename in enumerate(file_names):\n",
    "    cluster_label = labels[i]\n",
    "    source_path = os.path.join(folder_path, filename)\n",
    "    destination_path = os.path.join('Signature Fault Clusters Version 16 Final/VRM', f'VRM Cluster {cluster_label}', filename)\n",
    "    os.rename(source_path, destination_path)\n",
    "print(f\"Files successfully clustered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e227c-fa84-4ae7-9c80-1150bf498ffc",
   "metadata": {},
   "source": [
    "# To Find Accuracy of K Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024cc2ca-aeb9-41e2-a2bc-8833bc02f093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.3460\n",
      "Davies-Bouldin Index: 0.9572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Evaluate clustering without ground truth\n",
    "\n",
    "# Silhouette Score: Measures how similar a data point is to its cluster vs other clusters\n",
    "silhouette_avg = silhouette_score(combined_features, labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "\n",
    "# Davies-Bouldin Index: Measures the ratio of within-cluster distances to between-cluster distances\n",
    "db_index = davies_bouldin_score(combined_features, labels)\n",
    "print(f\"Davies-Bouldin Index: {db_index:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e0c84-8b8e-4154-883a-0ed6bc83de99",
   "metadata": {},
   "source": [
    "Rating for Clustering Performance\n",
    "The clustering performance can be evaluated as follows:\n",
    "\n",
    "1. Silhouette Score: 0.3460\n",
    "Interpretation:\n",
    "The Silhouette Score measures how similar data points are within their own cluster compared to other clusters.\n",
    "The score ranges from -1 to 1, where:\n",
    "> 0.5 indicates well-defined clusters.\n",
    "0.2 to 0.5 indicates moderately defined clusters.\n",
    "< 0.2 suggests overlapping or poorly defined clusters.\n",
    "A score of 0.3460 suggests that the clustering is moderately effective, but there is room for improvement in cluster separation.\n",
    "2. Davies-Bouldin Index (DBI): 0.9572\n",
    "Interpretation:\n",
    "DBI measures the average similarity between clusters, with lower values indicating better clustering.\n",
    "Typical ranges for DBI:\n",
    "< 1.0: Excellent clustering.\n",
    "1.0–1.5: Good clustering.\n",
    "> 1.5: Poor clustering.\n",
    "A DBI of 0.9572 indicates excellent clustering, where the clusters are compact and well-separated.\n",
    "Overall Rating\n",
    "Considering the moderate Silhouette Score and the excellent DBI, the clustering performance can be rated as \"Good\".\n",
    "While the DBI reflects strong inter-cluster separation, the moderate Silhouette Score suggests some clusters might still overlap or have less cohesion.\n",
    "Recommendation:\n",
    "To further improve performance:\n",
    "Tune the number of clusters (k): Experiment with different values for better cluster definitions.\n",
    "Feature Engineering: Explore additional or more relevant features for the dataset.\n",
    "Clustering Algorithm: Compare KMeans with other algorithms like DBSCAN or Gaussian Mixture Models for potentially better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280ea0b-d457-427a-acf0-820dd9592583",
   "metadata": {},
   "source": [
    "# CSV Files to Graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68300c3d-0cab-447e-9bdf-6c7152703e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs have been generated and stored in the output folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "csv_folder = 'Signature Fault Clusters Version 16 Final/VRM/VRM Cluster 9'\n",
    "output_folder = 'Signature Fault Clusters Version 16 Final/VRM Graph/VRM Cluster 9'\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over each CSV file in the folder\n",
    "for filename in os.listdir(csv_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_folder, filename)\n",
    "\n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Extract the relevant columns\n",
    "        if {'VRM'}.issubset(data.columns):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "\n",
    "            # Plot each voltage series\n",
    "            plt.plot(data.index, data['VRM'], label='VRM (Red Phase Voltage)', color='red')\n",
    "            # plt.plot(data.index, data['VYM'], label='VYM (Yellow Phase Voltage)', color='yellow')\n",
    "            # plt.plot(data.index, data['VBM'], label='VBM (Blue Phase Voltage)', color='blue')\n",
    "\n",
    "            # Adding labels and title\n",
    "            plt.xlabel('Time (Index)')\n",
    "            plt.ylabel('Voltage')\n",
    "            #plt.title(f'Voltage Measurements Over Time - {filename}')\n",
    "            #plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "            # Save the plot to the output folder\n",
    "            plot_filename = os.path.splitext(filename)[0] + '_voltage_plot.png'\n",
    "            plt.savefig(os.path.join(output_folder, plot_filename))\n",
    "            plt.close()\n",
    "\n",
    "        else:\n",
    "            print(f\"Columns VYM, VBM, VRM not found in {filename}\")\n",
    "\n",
    "print(\"Graphs have been generated and stored in the output folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d0442-3e03-4ff2-87a2-6917152efbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
